\chapter{Evaluation} \label{Evaluation}

In this section, the effectiveness of the proposed optimal control approach is tested in a simulation and compared it to previously used scenario theory. The simulation setup is described in section \ref{Setup}. The results of the OCP are shown in section \ref{optimal control}. Afterwards, we analyse the results and performance in more detail in section \ref{performance guarantees} by looking at the robustness of the solution and compare it to the commonly used scenario approach and finally we show the potential of this approach in section \ref{corridor}.

\section{Simulation Setup} \label{Setup}

We consider a system with the state transition function

\begin{equation}
\boldsymbol{f}(\boldsymbol{x}, u) = 
\begin{bmatrix}
0.8  x_1 - 0.5 x_2 \\
0.4 x_1 + 0.5 x_2 + u
\end{bmatrix}
\end{equation}

and the process noise distribution

\begin{equation}
\boldsymbol{v}_t \sim \mathcal{N} \left(\boldsymbol{0}, 
\begin{bmatrix}
0.03 & \text{-}0.004 \\
\text{-}0.004 & 0.01
\end{bmatrix}
\right).
\end{equation}

Both the state transition function and the process noise distribution are unknown to the user. Meanwhile, the observation function $g(\boldsymbol{x}, u) = x_1$ and measurement noise $w_t \sim \mathcal{N} (0, 0.1)$ is assumed to be known. This assumption can be made without a loss of genrality since an unknown observation model can be combined with the unknown transition model into a expanded model with $n_x + n_y$ states \cite{Frigola_15}.


For the scenario generation, we consider a set containing $T = 2000$ input and output measurements of the true system for our dataset $\mathbb{D}$. These measurements are obtained with a random input trajectory $u \sim \mathcal{N} (0, 3)$ while starting from a random initial state $\boldsymbol{x}_{\text{-}T} \sim \mathcal{N} ([2, 2]^\text{T}, \boldsymbol{I}_2)$. To infer the state model parameters, the approach from \cite{Svensson_17} is used. It is assumed that $\boldsymbol{f}(\cdot)$ is a linear combination of $n_a$ basis functions $\boldsymbol{\varphi}(\boldsymbol{x}_t, u_t)$ and the process noise is normally distributed. As such, the state transition can be rewritten as

\begin{equation} \label{State transition}
\boldsymbol{x}_{t+1} = \boldsymbol{A} \boldsymbol{\varphi}(\boldsymbol{x}_t, u_t) + \boldsymbol{v}_{t}
\end{equation}

with the basis functions $\boldsymbol{\varphi} (\boldsymbol{x}, u) = \left[ x_1,  x_2,  u \right]^\text{T}$, the process noise $\boldsymbol{v}_{t} \sim \mathcal{N} (\boldsymbol{0}, \boldsymbol{Q})$ and the unknown parameters $\boldsymbol{\theta}$ consisting of $\boldsymbol{A}$ and $\boldsymbol{Q}$. An inverse Wishart  prior with $l$ degrees of freedom and positive definite scale matrix $\Lambda$ is assumed for the matrix $\boldsymbol{Q}$. For the matrix $\boldsymbol{A}$ matrix normal prior with mean matrix $\boldsymbol{M} = \boldsymbol{0}$, right covariance $\boldsymbol{U} = \boldsymbol{Q}$ and left covariance matrix $\boldsymbol{V} \in \mathbb{R}^{n_a \times n_a}.$ For the estimation of the posterior disdribution with the PG sampler, we scale the basisvector with the weights $\left[ 0.1,  0.1,  1 \right]^\text{T}$ and for the prior the weights are chosen as $\boldsymbol{V} = 10 \boldsymbol{I}_5$.

We use gaussian kernels $k(x,y) = \text{exp}\left(\text{-}\frac{1}{2\sigma^2} ||x - y||_2^2 \right)$ with the bandwidth $\sigma$ set individually for all random parameters $\left\{\boldsymbol{x}_0^{[k]}, \boldsymbol{v}_{0:H}^{[k]}, w_{0:H}^{[k]},  \boldsymbol{A}^{[k]}\right\}$. As such, the elements of the Gram matrix $\boldsymbol{K} \in \mathbb{R}^{N \times N }$ are defined as

\begin{equation} \label{Kernel equation}
K_{ij} = k_{\sigma_A}(\boldsymbol{A}^{[i]}, \boldsymbol{A}^{[j]})  k_{\sigma_\mathcal{X}}(\boldsymbol{x}_0^{[i]}, \boldsymbol{x}_0^{[j]})    k_{\sigma_\mathcal{V}}(\boldsymbol{v}_{0:H}^{[i]}, \boldsymbol{v}_{0:H}^{[j]})  k_{\sigma_\mathcal{W}}(\boldsymbol{w}_{0:H}^{[i]}, \boldsymbol{w}_{0:H}^{[j]}).
\end{equation}

The individual elements of $\boldsymbol{\sigma} = [\sigma_A, \sigma_\mathcal{X}, \sigma_\mathcal{V}, \sigma_\mathcal{W}]$ are initialized via the median heuristic \cite{Damien_18} and then scaled to maximize the average probability of a set of $N_\text{test} = 2000$ test samples, i.e. find the $\boldsymbol{\sigma}$ that maximizes

 \begin{equation} \label{Average Probability}
\max\limits_{\sigma} \frac{1}{N_\text{test}}  \sum_{n= 1}^{N_\text{test}} \text{P}_{\boldsymbol{\sigma}} ( \boldsymbol{\delta_\text{test}}^{[n]} )
\end{equation}

with the probability function being the product of the four Gaussian probability functions 

 \begin{equation} \label{Gaussian Probability}
\text{P}_{\boldsymbol{\sigma}} ( \boldsymbol{\delta_\text{test}}^{[n]} ) = \prod_{\sigma_i \in \boldsymbol{\sigma}} \frac{1}{N_\text{train}} \sum_{m = 1}^{N_\text{train}} \frac{1}{\sqrt{2 \pi \sigma_i^2}} k_{\sigma_i}(\boldsymbol{\delta_\text{train}}^{[m]} ,\boldsymbol{\delta_\text{test}}^{[n]})
\end{equation}

over a set of $N_\text{train} = 300$ training samples that are independent of the set $N_\text{test}$.


\section{Optimal Control with Constrained Output} \label{optimal control}

In the following, we show how well the proposed optimal control approach works when applied to a OCP with constrained output by putting it side by side with the solution of the same problem where we have used the scenario approach which implements the chance-constraints by ensuring that the constraints are satisfied for every scenario $n \in \mathbb{N}_{\leq N}$ \cite{Garatti_22}. 

For this simulation, we are using scenarios that have been generated using the PG sampler. To this end, $16130$ samples were created and the first $N_p = 2000$ were discarded as training samples and the remaining samples were once again thinned with $n_d = 70$. The remaining $N = 200$ samples are then used as scenarios for the OCPs.

For the cost function, we consider a simple quadratic cost $J_H = \sum_{t = 0}^H u_t^2$ over the horizon $H = 40$. For constraints, we consider the input-constraint $\left| u \right| \leq 10$ as well as the temporarily active output-constraints $y_{10:20} \leq \text{-} 10$ and $10 \leq y_{30:40}$. $\varepsilon$ is chosen through Algorithm \ref{alg:Bootstrap} with a number of bootstrap samples $B = 1000$ and a confidence level $\beta = 0.95$. In this experiment, we look at the results for the risk level $\alpha$ being chosen as $0.01$, $0.2$ and $0.5$ to determine how much of an influence this parameter has on the solution.

The OCP can then be formulated as described in section \ref{Constraint Reformulation}. Since the problem has been chosen as convex in this example, a solution for both the scenario and kernel approach can be found easily by using a convex solver.

\begin{figure}[htb]
\centering
\subfigure[Scenario Approach ($\tilde{\alpha} = 0.16$)]{    %\alpha = 16.05%
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Scenario_K200_S2.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=40,
		ymin=-14, ymax=14,
		xtick={0, 10, 20, 30, 40},
		ytick={-10, -5, 0, 5, 10},
		ylabel=$y$, xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\addplot[name path=B, thick, opacity=0.2] table[x=t,y=y_opt_min]{\file};
		\tikzfillbetween[of=A and B]{opacity=0.2};
		\addlegendentry{$\left\{y_{0:H}^{[1:N]} \right\}$}
		
		\addplot[ultra thick,black!20!green] table[x=t,y=y_pred]{\file};
		\addlegendentry{$\frac{1}{N}\sum\limits_{n=1}^N y_{0:H}^{[n]}$}
		
		\addplot[ultra thick, blue] table[x=t,y=y_true]{\file};
		\addlegendentry{$y_{0:H}$}
		
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (10,15) rectangle (20,-10); 
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 }
%\quad % puts next subfigure right next to the previous subfigure
\subfigure[Kernel Approach ($\alpha = 0.01$)]{
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Kernel_K200_Alpha001_S2.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=40,
		ymin=-14, ymax=14,
		xtick={0, 10, 20, 30, 40},
		ytick={-10, -5, 0, 5, 10},
		%ylabel=$y$, 
		xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\addplot[name path=B, thick, opacity=0.2] table[x=t,y=y_opt_min]{\file};
		\tikzfillbetween[of=A and B]{opacity=0.2};
		\addlegendentry{$\left\{y_{0:H}^{[1:N]} \right\}$}
		
		\addplot[ultra thick,black!20!green] table[x=t,y=y_pred]{\file};
		\addlegendentry{$\frac{1}{N}\sum\limits_{n=1}^N y_{0:H}^{[n]}$}
		
		\addplot[ultra thick, blue] table[x=t,y=y_true]{\file};
		\addlegendentry{$y_{0:H}$}
		
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (10,15) rectangle (20,-10); 
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 } 

\subfigure[Kernel Approach ($\alpha = 0.2$)]{
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Kernel_K200_Alpha02_S2.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=40,
		ymin=-14, ymax=14,
		xtick={0, 10, 20, 30, 40},
		ytick={-10, -5, 0, 5, 10},
		ylabel=$y$, xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\addplot[name path=B, thick, opacity=0.2] table[x=t,y=y_opt_min]{\file};
		\tikzfillbetween[of=A and B]{opacity=0.2};
		\addlegendentry{$\left\{y_{0:H}^{[1:N]} \right\}$}
		
		\addplot[ultra thick,black!20!green] table[x=t,y=y_pred]{\file};
		\addlegendentry{$\frac{1}{N}\sum\limits_{n=1}^N y_{0:H}^{[n]}$}
		
		\addplot[ultra thick, blue] table[x=t,y=y_true]{\file};
		\addlegendentry{$y_{0:H}$}
		
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (10,15) rectangle (20,-10); 
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 }
%\quad % puts next subfigure right next to the previous subfigure
\subfigure[Kernel Approach ($\alpha = 0.5$)]{
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Kernel_K200_Alpha05_S2.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=40,
		ymin=-14, ymax=14,
		xtick={0, 10, 20, 30, 40},
		ytick={-10, -5, 0, 5, 10},
		%ylabel=$y$, 
		xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\addplot[name path=B, thick, opacity=0.2] table[x=t,y=y_opt_min]{\file};
		\tikzfillbetween[of=A and B]{opacity=0.2};
		\addlegendentry{$\left\{y_{0:H}^{[1:N]} \right\}$}
		
		\addplot[ultra thick,black!20!green] table[x=t,y=y_pred]{\file};
		\addlegendentry{$\frac{1}{N}\sum\limits_{n=1}^N y_{0:H}^{[n]}$}
		
		\addplot[ultra thick, blue] table[x=t,y=y_true]{\file};
		\addlegendentry{$y_{0:H}$}
		
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (10,15) rectangle (20,-10); 
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 } 
\caption{Example of the optimal control with known basis functions for scenario approach (top left) and kernel approach for various values of $\alpha$. The red areas show the output constraints. The gray area encompasses the 200 scenarios that were used in the optimization with the green line being the average. The blue line is one realization of the true output.}
\label{ScenarioKernelComparison}
\end{figure}



The results of an exemplary run is shown in Figure \ref{ScenarioKernelComparison}. The figure includes the four plots for the scenario and kernel approach for each $\alpha$ and shows the output $y$ of their respective OCPs. The graphs shows the spread of the $N = 200$ trajectories that are generated when the input $\boldsymbol{u}_{0:H}$ is applied to the scenarios that were used to find the optimal input. On top that, it also shows of the mean of these trajectories and true output. Where the graphs differ however is to what extend the solutions fulfill the constraints. By its definition, the scenario approach requires all scenarios to fulfill the constraints which can be seen in the solution. While the gray area touches the lower and upper bounds during several timesteps, it never violates the constraints. The kernel approach on the other hand has a risk factor $\alpha$ built in which allows for a number of scenarios to violate the constraints as long a sufficient number satisfies them. This can be seen by the gray area following a similar trajectory to the scenario approach while a small portion of the area actively overlaps with the marked area. This is especially apparent in the plots with larger $\alpha$ values. In contrast, the graph where $\alpha$ is chosen as $0.01$ is almost identical to the scenario approach. These results show that the kernel approach allows us to find solutions outside the feasible region of the scenario approach and potentially find a solution with a lower cost. This is done in exchange for an increased risk of the true output violating one or more of the constraints which can be seen by the blue line being closer to the constraints. A particularly close example can be seen at the end of the plot at $t = 40$ where the true output comes very close to the constraints for $\alpha = 0.5$ but stays further away for the scenario approach or $\alpha = 0.01$.




\section{Robustness} \label{performance guarantees}

The biggest advantage that this kernel approximation has shown compared to the scenario approach is the adjustable risk factor. This parameter $\alpha \in [0, 1]$ can be chosen depending on how successful the final solution is supposed to be when it comes to satisfying the constraints in future scenarios. 

In this section, this parameter is tested by running the same problem setup as was used in section \ref{optimal control} for different values of $\alpha$ as well as increasing and lowering the number of samples $N$ and testing how well the solution holds up for other scenarios that are independent of the ones used in the optimization.

Similar to section \ref{optimal control}, a number of scenarios are generated with Algorithm \ref{alg:PGibbs}. From this set of scenarios, a small subset is then taken and used to formulate several OCPs as was already described in the previous section. The OCPs are then solved and the resulting optimal input $\boldsymbol{u}_{0:H}$ is applied to $N = 2000$ more independent scenarios from the same system to test how well this solution holds up. For each of the 2000 scenarios, the output is calculated and compared to the constraints that were used in the OCP to check whether or not they are fulfilled. This process is then repeated for various numbers of scenarios. It is done for 1, 5, 10, 25 and 50 samples and then increased with a step size of 50 until $N = 300$ samples are used in the optimization.

\begin{figure}[t]
		\pgfplotsset{width=13cm, compat = 1.18, 
			height = 9cm, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/AlphaTest_K300_MaxConstraint_S2.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=both,
		xmin=1, xmax=300,
		ymin=0, ymax=100,
		xtick={0, 50, 100, 150, 200, 250, 300},
		ytick={0, 20, 40, 60, 80, 100},
		ylabel={Constraints Satisfied [\%]}, xlabel=$N$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		\addplot[thick,black!20!red] table[x=K,y=Kernel04]{\file};
		\addlegendentry{Kernel Approach ($\alpha = 0.4$)}
		
		\addplot[thick,black!20!orange] table[x=K,y=Kernel03]{\file};
		\addlegendentry{Kernel Approach ($\alpha = 0.3$)}

		\addplot[thick,black!20!yellow] table[x=K,y=Kernel02]{\file};
		\addlegendentry{Kernel Approach ($\alpha = 0.2$)}

		\addplot[thick,black!20!green] table[x=K,y=Kernel01]{\file};
		\addlegendentry{Kernel Approach ($\alpha = 0.1$)}

		\addplot[thick,black!20!blue] table[x=K,y=Scenario]{\file};
		\addlegendentry{Scenario Approach}
		\end{axis}
		\end{tikzpicture}
		\vspace*{-0.4cm}
		
		\caption{Percentage of scenarios where $u_{0:H}$ is a feasible solution. The blue line shows the result of the scenario approach while the other lines are for the kernel approach with various values of $\alpha$.}
		\label{fig:robustness_plot}
\end{figure}


In Figure \ref{fig:robustness_plot} the results of this simulation are shown. The percentage of scenarios that fulfill the constraints is plotted over the number of samples used in the initial optimization which range from $N = 1$ to 300. The various $\alpha$ values are shown as separate lines. Initially, all five plots show very similar results. This can be explained by the fact that at such a low number of scenarios cannot accurately represent the distribution. As the number of scenarios is increased, the approximation of the distribution becomes better leading to a higher percentage of scenarios where the constraints are satisfied.

After around 25 scenarios, the plots start diverging for the first time. While the scenario approach and the plots with smaller $\alpha$ values are very similar, the lines that represent larger $\alpha$ values are starting to display a slightly different percentage of cases that satisfy the constraints. It can be seen that the kernel approach achieves a more robust solution for $\alpha = 0.3$ and $N = 50$ but since this effect is only present for this one instance, it can be inferred that this is most likely just due to random chance.

For $N \geq 100$ scenarios, a trend starts to become apparent in the different lines as all of them start to slowly converge with the risk factor $\alpha$ determining to what percentage each line converges. An interesting oddity that can be seen here is that the 3 lines for $\alpha = 0.2, 0.3$ and $0.3$ all display the same pattern of their percentage increasing slighly above the rest of the curves before they drop below the rest. After that, the lines converge to various values that are dependent on the value $\alpha$ that's associated with each line.

Meanwhile, the scenario approach is displaying a similar behavior with the percentage of scenarios that satisfy the constraints improving as the number of scenarios used for the optimization is increased. Something that stands out here is that, at least within the observed range of samples used in the optimization, the kernel approach seemingly provides a more reliable solution $\boldsymbol{u}_{0:H}$ than the scenario approach. This shows clearly, that even if the kernel approach begins with the goal to allow for portion of the scenarios to not satisfy the constraints, it can still lead to a more reliable solution than the scenario approach which by its definition tries to satisfy the constraints of as many scenarios as possible.

As the number of scenarios used in the optimization keeps increasing, first signs of their behavior for $N \to \infty$ become visible. While we require more samples to be sure of the exact percentages the approaches are converging to, it is already clear that they are not converging towards $(1 - \alpha)$ as we has hoped. For example, for $\alpha = 0.4$ it seems to converge to somewhere between 85\% and 90\%, while the same example converges towards somewhere around 95\% for $\alpha = 0.1$.
 
\section{Corridor Test} \label{corridor}

In section \ref{performance guarantees} it has been shown that the kernel approach can be used to control allows us to control the risk factor $\alpha$, leading to a higher percentage of cases violating the constraints. The benefit of this becomes apparent when looking at the cost function $J_H$. Allowing a higher risk relaxes the constraints which allows us to choose a solution with a lower cost associated with it to satisfy the constraints. While this can be shown for the example used in section \ref{optimal control} and \ref{performance guarantees}, the potential of this approach becomes more clear by looking at an example where the possible solutions are seperated into two paths, one with a low costs and hard to satisfy constraints and one with low costs and easy to satisfy constraints.

In this example, we consider the same system as described in section \ref{Setup} and \ref{optimal control} where the horizon has been reduced to $H = 20$ and the output $y$ has to either satisfy $-5 \leq y_t \leq -2.5$ or $y_t \leq -5 - \sqrt{25- (t - 10)^2}$ for $t \in [5, 15]$. We use 2000 scenarios that were generated via Particle Gibbs and randomly select $N = 100$ scenarios to formulate and solve an optimal control problem to see. The resulting output can then be applied to the true system to obtain a true output which can be analyzed regarding which path it takes and whether or not it satisfies the constraints. This process was repeated 50 times for a different set of randomly chosen scenarios for both the scenario and kernel approach for the risk values $\alpha = 0.1$, $0.2$ and $0.3$.


\begin{figure}[htb]
\centering
\subfigure[Scenario Approach]{
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Scenario_K100_N50.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=20,
		ymin=-12, ymax=4,
		xtick={0, 5, 10, 15, 20},
		ytick={-12, -8, -4, 0, 4},
		ylabel=$y$, xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		%\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\foreach \index in {1,...,47}
			\addplot[thin, blue] table[x=t,y=Successful\index]{\file};
		%\addlegendentry{$y_{0:H}$}
		\foreach \index in {1,...,3}
			\addplot[thin, red] table[x=t,y=Failed\index]{\file};

		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,-5) -- (15,-5) arc(0:-180:5) --cycle;
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,15) rectangle (15,-2.5); 
		%\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 }
\subfigure[Kernel Approach ($\alpha = 0.1$)]{
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Kernel_K100_N50_Alpha01.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=20,
		ymin=-12, ymax=4,
		xtick={0, 5, 10, 15, 20},
		ytick={-12, -8, -4, 0, 4},
		ylabel=$y$, xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		%\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\foreach \index in {1,...,47}
			\addplot[thin, blue] table[x=t,y=Successful\index]{\file};
		%\addlegendentry{$y_{0:H}$}
		\foreach \index in {1,...,3}
			\addplot[thin, red] table[x=t,y=Failed\index]{\file};

		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,-5) -- (15,-5) arc(0:-180:5) --cycle;
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,15) rectangle (15,-2.5); 
		%\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 }
\subfigure[Kernel Approach ($\alpha = 0.2$)]{
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Kernel_K100_N50_Alpha02.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=20,
		ymin=-12, ymax=4,
		xtick={0, 5, 10, 15, 20},
		ytick={-12, -8, -4, 0, 4},
		ylabel=$y$, xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		%\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\foreach \index in {1,...,48}
			\addplot[thin, blue] table[x=t,y=Successful\index]{\file};
		%\addlegendentry{$y_{0:H}$}
		\foreach \index in {1,...,2}
			\addplot[thin, red] table[x=t,y=Failed\index]{\file};

		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,-5) -- (15,-5) arc(0:-180:5) --cycle;
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,15) rectangle (15,-2.5); 
		%\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 }
%\quad % puts next subfigure right next to the previous subfigure
\subfigure[Kernel Approach ($\alpha = 0.3$)]{
\pgfplotsset{width=.47\textwidth, compat = 1.18, 
			height = .4\textwidth, grid= major, 
			legend cell align = left, ticklabel style = {font=\scriptsize},
			every axis label/.append style={font=\scriptsize},
			legend style = {font=\scriptsize},
        }
		\def\file{data/Kernel_K100_N50_Alpha03.txt}
		
		\centering
		\begin{tikzpicture}
		\begin{axis}[
		grid=none,
		xmin=0, xmax=20,
		ymin=-12, ymax=4,
		xtick={0, 5, 10, 15, 20},
		ytick={-12, -8, -4, 0, 4},
		ylabel=$y$, xlabel=$t$,
		set layers=standard,
		reverse legend,
		legend style={font=\scriptsize, at={(1,0)},anchor=south east, row sep=2pt},
		ylabel shift = -6 pt]
		
		%\addplot[name path=A, forget plot, thick, opacity=0.2] table[x=t,y=y_opt_max]{\file};
		\foreach \index in {1,...,47}
			\addplot[thin, blue] table[x=t,y=Successful\index]{\file};
		%\addlegendentry{$y_{0:H}$}
		\foreach \index in {1,...,3}
			\addplot[thin, red] table[x=t,y=Failed\index]{\file};

		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,-5) -- (15,-5) arc(0:-180:5) --cycle;
		\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (5,15) rectangle (15,-2.5); 
		%\draw [fill=red, fill opacity=0.2,red, opacity=0.2] (30,10) rectangle (40,-15); 
		\end{axis}
		\end{tikzpicture}
 }
\caption{Example of the optimal control with known basis functions for scenario approach (top left) and kernel approach. The red areas show the output constraints. The lines show trajectories of the true system. The outputs that successfully avoid the constraints are colored blue, while all the outputs that violate at least one constraint are colored red.}

\label{ScenarioKernelComparisonCorridor}
\end{figure}

In Fig. \ref{ScenarioKernelComparisonCorridor} the true outputs for these 50 runs have been plotted as well as the constraints. The runs that arrive at a feasible solution have been colored blue and the failed runs, i.e. the runs where the true output violated at least one constraint, have been colored red. 

Looking at the scenario approach, it can be seen that almost all of the runs end up choosing the less risky path that comes with a higher cost as can be seen in table \ref{tab:results_corridor} where the average cost of 50 runs is shown.  In contrast, we can look at the kernel approach for $\alpha = 0.3$ where only a very small portion of the runs take the less risky path despite the same scenarios being used for both approaches. This is also reflected in the average cost which is significantly lower for the kernel approach. Here, the cost is directly proportional to the percentage of runs that take the safe path. The table also shows the percentage of runs where the true output violated a constraint at least once. 

%This cost reduction is however also tied with a higher percentage of failed runs for the kernel approach which can be seen in the bottom row of table \ref{tab:results_corridor}.

The results for $\alpha = 0.1$ and $\alpha = 0.2$ are also interesting as they show a middle ground between the two extremes. We can see that as $\alpha$ is increased, the number of runs that goes through the risky corridor increases which leads to a lower average cost and a higher number of failed runs. 


\begin{table}
\centering
\begin{tabular}{|c| c| c| c| c|}
\hline
Approach & Scenario &  \multicolumn{3}{|c|}{Kernel}\\  \cline{3-5} & &  $\alpha = 0.1$ & $\alpha = 0.2$ & $\alpha = 0.3$ \\
\hline
$J_H$ (mean) & $338.4$ & $255.9$ & $129.5$ & $69.9$\\
\hline
Risky Runs & $8$\% & $40$\% & $78$\% & $98$\% \\
\hline
Failed Runs & $6$\% & $6$\% & $4$\% & $6$\% \\
\hline
\end{tabular}
\caption{Results of the 250 examplary runs with known basis functions for the scenario approach and kernel approach for risk values of $0.1$, $0.2$ and $0.3$. The table shows the average cost of the 250 runs and the percentage of failed Monte Carlo runs, i.e. runs where the true output violated at least one constraint.}
\label{tab:results_corridor}
\end{table} 

%Data 250 Runs
% J_H   317.2   265.4   117.4   65.6
%Risky Runs 16.4% 36.8% 17.2% 99.2%
%Failed Runs  0.8960    0.9200    0.9440    0.9120