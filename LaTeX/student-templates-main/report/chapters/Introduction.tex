%_________Einleitung__________________________________
\chapter{Introduction}
\label{sec:introduction}

In the realm of model-based control, the development of reliable and accurate mathematical models is crucial to ensuring the success of any control application. However, finding such a model based on physical knowledge is often very time-consuming or even impossible. Because of this, data-driven control approaches that allow to derive such models based on previously collected data are gaining attention and their usefulness for optimal control applications is being explored. For safety-critical applications, it is especially important to be able to take into account uncertainties that result from basing your model on a limited set of training data (epistemic uncertainty) and the inherent randomness of the system such as measurement noise (aleatory uncertainty). To this end, there are already numerous methods, such as a combination of state-space models and Gaussian processes \cite{Williams_06} or similar Bayesian approaches, that allow for reliable quantifications of uncertain models.

These approaches do, however, require full-state measurements of the unknown system in order to learn the model, quantify uncertainties, and make predictions, which can be disadvantageous as these measurements are not available in numerous practical applications. In many applications, it is unknown which variables directly affect the system, or some states are not measurable. While methods such as LARX approaches \cite{Maiworm_21} exist, that do not require the state space representation, they come with the disadvantage of it not being clear how to include the underlying physics of the model in the learning process. Having no knowledge of the states can also, for example, lead to these methods being unable to distinguish between the noise that permanently affects the system (process noise) and the noise that only affects one specific measurement (measurement noise). 

This means it is often advantagous to keep working with state-space models and in the case of latent states try to overcome the reliance on state measurements by jointly estimating the unknown dynamics and the latent states to create a reliable model for the system. This idea is already being utilized for optimal control in \cite{Robert_24} to estimate the uncertain elements of the system and use the acquired data combined with scenario theory \cite{Garatti_22} to obtain a robust solution. However, with this approach, the performance guarantees, i.e., a lower bound for the probability that the solution is feasible, have to be determined retroactively with a computationally complex process.
 
In this thesis, we propose how the use of kernel embeddings \cite{Yassine_22} as a way to reformulate the chance-constrained problem by using samples drawn from an distribution, that is generally is analytically intractable. This is possible by using particle Markov chain Monte Carlo (PMCMC) methods which allow us to jointly estimate dynamics and latent states. This enables us to include the allowed risk factor in the optimization process as a way to set performance guarantees in advance.

In this chapter, the problem is introduced in Section \ref{Problem Statement}, and related work is discussed in Section \ref{Related Work}. Finally, a quick overview of the structure of the rest of this thesis is given in Section \ref{Structure of this Thesis}.

\section{Problem Statement} \label{Problem Statement}

Consider the general nonlinear discrete-time system of the form

\begin{subequations} \label{System equation}
\begin{equation}
\boldsymbol{x}_{t+1} = \boldsymbol{f} \left( \boldsymbol{x}_{t}, \boldsymbol{u}_t \right) + \boldsymbol{v}_{t}
\end{equation}
\begin{equation}
\boldsymbol{y}_{t} = \boldsymbol{g} \left( \boldsymbol{x}_{t}, \boldsymbol{u}_t \right) + \boldsymbol{w}_{t}
\end{equation}
\end{subequations}

with the state $\boldsymbol{x}_t \in \mathbb{R}^{n_x \in \mathbb{N}}$, the input $\boldsymbol{u}_t \in \mathbb{R}^{n_u \in \mathbb{N}}$, the output $\boldsymbol{y}_t \in \mathbb{R}^{n_y \in \mathbb{N}}$, the process noise $\boldsymbol{v}_{t} \in \mathbb{R}^{n_x}$, the measurement noise $\boldsymbol{w}_{t} \in \mathbb{R}^{n_y}$ and time $t \in \mathbb{Z}$. 

In our setting, only the output $\boldsymbol{y}_t$ is observed and the state transition function $\boldsymbol{f}(\cdot)$ and the observation function $\boldsymbol{g}(\cdot)$, as well as the distributions $\boldsymbol{\mathcal{V}}$ and $\boldsymbol{\mathcal{W}}$ of the process noise $\boldsymbol{v}_t$ and measurement noise $\boldsymbol{w}_t$ are unknown.

We assume that a dataset $\mathbb{D} = \left\{\boldsymbol{u}_{t}, \boldsymbol{y}_{t}\right\}_{t = \text{-}T:\text{-}1}$ containing the last $T \in \mathbb{N}$ measurements of the input $\boldsymbol{u}_t$ and output $\boldsymbol{y}_t$ is available.

We further assume that the structure of the model $\left\{\boldsymbol{f}_{\boldsymbol{\theta}}(\cdot), \boldsymbol{g}_{\boldsymbol{\theta}}(\cdot), \boldsymbol{\mathcal{V}}_{\boldsymbol{\theta}}, \boldsymbol{\mathcal{W}}_{\boldsymbol{\theta}}\right\}$ is known, for example from a physical insight into the system, and is dependent on a finite number of unknown parameters $\boldsymbol{\theta}$. In addition to that, the priors $p(\boldsymbol{\theta})$ and $p(\boldsymbol{x}_{\text{-}T})$ are available as well.

The objective is to minimize a given cost function 

\begin{equation} \label{cost function}
J_H = \sum_{t = 0}^H c(\boldsymbol{u}_t,  \boldsymbol{x}_t,  \boldsymbol{y}_t)
\end{equation}

over the horizon $H$ while satisfying the constraints 

\begin{equation} \label{constraints}
\boldsymbol{h}(\boldsymbol{u}_{0:H},  \boldsymbol{x}_{0:H},  \boldsymbol{y}_{0:H}) \leq \boldsymbol{0}
\end{equation}

with $\boldsymbol{h} \in \mathbb{R}^{n_c}$ being a vector of arbitrary deterministic function. As the states $\boldsymbol{x}_{0:H}$ are unknown and there are several uncertain factors in our system, i.e., the process and measurement noise and the unknown parameter $\boldsymbol{\theta}$ that characterizes the system, the constraints are transformed into chance-constraints where only a portion of the possible cases have to satisfy the constraints. This is done due to the possibility that $\boldsymbol{h}$ is impossible to satisfy for every possible $\boldsymbol{x}_{0:H}$. For this, we also introduce a risk factor $\alpha \in [0, 1]$ that relaxes the constraints, turning them into

\begin{equation} \label{risk constraints}
P \left[ \text{max} (\boldsymbol{h}(\boldsymbol{u}_{0:H},  \boldsymbol{x}_{0:H},  \boldsymbol{y}_{0:H})) \leq 0 \right] \geq 1 - \alpha
\end{equation}

with the underlying distribution of the data being generally unknown.

\section{Related Work} \label{Related Work}

%From Robert's \emph{Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States}\cite{Robert2024}:

The problem presented in Section \ref{Problem Statement} provides several challenges as the information we have available is very limited. While many methods to solve chance-constrained problems exist, they often rely on knowledge of the posterior distribution, which is generally unknown in our problem. While we do have priors for the uncertain elements, we only have access to the prior for $\boldsymbol{x}_{\text{-}T}$ and the forward propagation over $T$ timesteps will generally lead to the optimization problem becoming infeasible due to the high degree of uncertainty. As such, the priors must be updated based on the input-output measurements $\mathbb{D}$, i.e., the posterior distribution must be inferred, but this distribution is analytically intractable \cite{Andrieu_10}. To draw samples from the posterior distribution, particle Markov chain Monte Carlo (PMCMC) methods were proposed \cite{Andrieu_10}. 

This has recently been exploited for optimal control in \cite{Robert_24}, utilizing such a sampler to generate scenarios that describe possible future trajectories of the unknown system. These scenarios are then used to formulate a deterministic optimal control problem by reformulating the chance constraints with the scenarios \cite{Garatti_22}. However, this usage of the scenarios comes with the drawback that the risk factor cannot be specified for the final optimal control problem (OCP), and the process of estimating it retroactively is resource-intensive.

As such, there is a need to find other methods that allow us to utilize the samples generated by the PMCMC sampler to reformulate the chance constraints to find a distributionally robust solution without losing the risk factor in the process. 

As the difficulties with this can be traced back to the unknown distribution, kernel distribution embeddings have been proposed in \cite{Adam_21} and \cite{Adam_22} to reformulate chance-constrained control optimal control problems. However, these approaches work under the assumption that the states are known as the transition function is embedded directly. As such, this approach is unsuited for the latent states we are working with in our problem.

Another workaround that has been proposed is the use of ambiguity sets. Here, ambiguity sets are defined as a set of probability distributions that are within a certain radius under an appropriate distance metric. For this purpose, Wasserstein distance was proposed as a metric for the ambiguity set in \cite{Hota_19}. It has, however, been proven rather difficult to efficiently construct a Wasserstein ambiguity set for problems with works limiting themselves to affine constraint functions. 

Another metric is proposed in \cite{Yassine_22} allows for an efficient construction of an ambiguity set using a maximum mean discrepancy (MMD) metric combined with kernel approximation. In contrast to Wasserstein ambiguity sets, this approach can be applied to general nonlinear and nonconvex constraints. 

In this thesis, MMD ambiguity sets are combined with PMCMC sampling methods to solve an OCP. In contrast to the scenario theory, this allows for the reformulation of the chance constraints while including the risk factor as a way to set performance guarantees before solving the problem.

\section{Structure of this Thesis} \label{Structure of this Thesis}

The remainder of this paper is structured as follows. In Chapter \ref{Technical Background}, we review the methods used to create a PMCMC sampler and how the OCP is reformulated with scenario theory. Following that, we describe the alternative approach using ambiguity sets in Chapter \ref{Technical Approach}. These methods are then tested and evaluated in Chapter \ref{Evaluation}. Finally, the results are summarized, and some concluding remarks are given in Chapter \ref{Conclusion}.



%____________________________________________________