%_________Einleitung__________________________________
\chapter{Introduction}
\label{sec:introduction}

Your first chapter in the document.
Introduce the problem (gently!). Try to give the reader an appreciation of the difficulty, and an idea of how you will go about it. It is like the overture of an opera: it plays on all the relevant themes.

Make sure you clearly state the vision/aims of your work, which problem you are trying to solve, and why it is important. While the introduction is the part that is read first (ignoring title and abstract), it is usually best written last (when you actually know what you have really achieved). Remember, it is the first thing that is read and will have a major influence on the how the reader approaches your work. If you bore them now, you have most likely lost them already. If you make outrageous claims, pretend to solve the world's problems, etc, you are likely fighting an uphill battle later on. Also, make sure you pick up any threads spun in the introduction later on, to ensure that the reader thinks they get what they have been promised. Do not create an expectation that you will deliver more than you actually do. Remember, the reader may be your marker (of a thesis) or referee (of a paper), and you do not want to annoy them.

\section{Problem Statement} \label{Problem Statement}

Consider the general nonlinear discrete-time system of the form

\begin{subequations} \label{System equation}
\begin{equation}
\boldsymbol{x}_{t+1} = \boldsymbol{f} \left( \boldsymbol{x}_{t}, \boldsymbol{u}_t \right) + \boldsymbol{v}_{t}
\end{equation}
\begin{equation}
\boldsymbol{y}_{t} = \boldsymbol{g} \left( \boldsymbol{x}_{t}, \boldsymbol{u}_t \right) + \boldsymbol{w}_{t}
\end{equation}
\end{subequations}

with the state $\boldsymbol{x} \in \mathbb{R}^{n_x \in \mathbb{N}}$, the input $\boldsymbol{u} \in \mathbb{R}^{n_u \in \mathbb{N}}$, the output $\boldsymbol{y} \in \mathbb{R}^{n_y \in \mathbb{N}}$, the process noise $\boldsymbol{v}_{t} \in \mathbb{R}^{n_x}$, the measurement noise $\boldsymbol{w}_{t} \in \mathbb{R}^{n_y}$ and time $t \in \mathbb{Z}$. 

In our setting, only the output $\boldsymbol{y}$ is observable and the state transition function $\boldsymbol{f}(\cdot)$ and the observation function $\boldsymbol{g}(\cdot)$, as well as the distributions $\boldsymbol{\mathcal{V}}$ and $\boldsymbol{\mathcal{W}}$ of the process noise $\boldsymbol{v}$ and measurement noise $\boldsymbol{w}$ are unknown.

We assume that a dataset $\mathbb{D} = \left\{\boldsymbol{u}_{t}, \boldsymbol{y}_{t}\right\}_{t = \text{-}T:\text{-}1}$ containing the last $T \in \mathbb{N}$ measurements of the input $\boldsymbol{u}$ and output $\boldsymbol{y}$ is available.

We further assume that the structure of the model $\left\{\boldsymbol{f}_{\boldsymbol{\theta}}(\cdot), \boldsymbol{g}_{\boldsymbol{\theta}}(\cdot), \boldsymbol{\mathcal{V}}_{\boldsymbol{\theta}}, \boldsymbol{\mathcal{W}}_{\boldsymbol{\theta}}\right\}$ is known and is dependent on a finite number of unknown parameters $\boldsymbol{\theta}$. In addition to that, the priors $p(\boldsymbol{\theta})$ and $p(\boldsymbol{x}_{\text{-}T})$ are available as well.

The objective is to minimize a given cost function 

\begin{equation} \label{cost function}
J_H = \sum_{t = 0}^H c(\boldsymbol{u}_t,  \boldsymbol{x}_t,  \boldsymbol{y}_t)
\end{equation}

over the horizon $H$ while satisfying the constraints 

\begin{equation} \label{constraints}
\boldsymbol{h}(\boldsymbol{u}_{0:H},  \boldsymbol{x}_{0:H},  \boldsymbol{y}_{0:H}) \leq \boldsymbol{0}
\end{equation}

with $\boldsymbol{h} \in \mathbb{R}^{n_c}$ being a vector of arbitrary deterministic function. As the states $\boldsymbol{x}_{0:H}$ are unknown to us and there are several uncertain factors in our system, the constraints are transformed into chance-constraints and since it is possible that $\boldsymbol{h}$ is impossible to satisfy for every possible $\boldsymbol{x}_{0:H}$, we also introduce a risk factor $\alpha \in [0, 1]$ that relaxes these constraints, turning them into

\begin{equation} \label{risk constraints}
P \left[ h_i(\boldsymbol{u}_{0:H},  \boldsymbol{x}_{0:H},  \boldsymbol{y}_{0:H}) \leq 0 \right] \geq 1 - \alpha, \; \forall i = 1,...,n_c
\end{equation}

with $h_i$ being the $i$-th element of $\boldsymbol{h}$ and $P$ being generally unknown.

\section{Related Work}

%From Robert's \emph{Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States}\cite{Robert2024}:

To solve the problem defined in Sec. \ref{Problem Statement}

One such method is to use particle Markov chain Monte Carlo Methods \cite{Andrieu_10} to draw samples for the unknown dynamics and system trajectory. This has recently been used in \cite{Robert_24} for optimal control , utilizing such a sampler to generate scenarios for the system and using these scenarios as a representation of the unknown distribution to formulate a deterministic optimal control problem by reformulating the chance-constraints. However, the usage of the scenarios in this paper comes with the major drawback of the risk factor $\alpha$ not being part of the final optimal control problem (OCP) and the process of estimating it retroactively being quite ressource intensive.

As such, there is a need to find other methods that allow us to utilize the samples generated by PG sampler to reformulate the chance-constraints to find a distributionally robust solution without losing the risk factor in the process. As the difficulties with this can be traced back to the unknown distribution $P$, ambiguity sets have been proposed as a possible workaround. Here, ambiguity sets are defined as a set of probability distributions that are within a certain radius under an appropriate distance function. This was used in \cite{Hota} with Wasserstein distance as the metric for the ambiguity set. It has however been proven rather difficult to efficiently construct a Wasserstein ambiguity set for problems. 

In contrast, the metric proposed in \cite{Yassine_22} allows for an efficient construction of an ambiguity set using a maximum mean discrepancy (MMD) metric combined with kernel approximation.

The remainder of this paper is structured as follows. In chapter \ref{Technical Approach}, we review the methods used to solve the OCP. These methods are then tested and evaluated in chapter \ref{Evaluation}. And finally, the results are summarized and some concluding remarks are given in chapter \ref{Conclusion}.





In addition to the obvious purpose indicated, the related work section also can serve to:

\begin{itemize}
	\item justify that the problem exists by example and argument
	\item motivate interest in your work by demonstrating relevance and importance
	\item identify the important issues
	\item provide background to your solution
\end{itemize}

Any remaining doubts over the existence, justification, motivation, or relevance of your thesis topic or problem at the end of the introduction should be gone by the end of related work section.

Note that a literature review is just that, a review. It is not a list of papers and a description of their contents! A literature review should critique, categorize, evaluate, and summarize work related to your thesis. Related work is also not a brain dump of everything you know in the field. You are not writing a textbook; only include information directly related to your topic, problem, or solution.''

Note: Write literature review at an early stage of your project to build on the knowledge of others, not reinvent the wheel over and over again! There is nothing more frustrating after weeks or months of hard work to find that your great solution has been published 5 years ago and is considered old news or that there is a method known that produces superior results.

\cite{Robert_24}

\cite{Yassine_22}

\cite{Adam_22}
%____________________________________________________